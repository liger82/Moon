---
layout: post
title: "[paper review] Deterministic Policy Gradient"
date: 2021-04-24
excerpt: "DPG는 결정론적이면서 off-policy actor-critic을 사용해서 탐험의 가능성도 열어놓았다."
categories: [RL/RL]
tags : [Reinforcement Learning, RL, DPG, Deterministic Policy Gradient]
comments: true
---

> <subtitle> Abstract </subtitle>

* paper : [논문 링크](http://proceedings.mlr.press/v32/silver14.pdf){:target="_blank"}  

Deterministic Policy Gradient(DPG) 는 continuous actions 을 다루는 강화학습 알고리즘으로 Q function의 expected gradient 형태로 되어 있어서 Stochastic Policy Gradient(SPG) 보다 data efficiency 측면에서 이점을 지닌다.  
결정론적이기 때문에 탐험(exploration)이 상대적으로 적어질 수 있다. 이를 극복하기 위해 off-policy actor-critic 알고리즘을 사용한다. 결과적으로 DPG를 사용하여 고차원의 행동 공간에서 SPG보다 더 좋은 성능을 낼 수 있도록 하는 것이 목적이다.

<br>

---

<br>

> <subtitle> Introduction </subtitle>

Value-based method는 제한된 숫자의 비연속적인 행동에서만 동작할 수 있기 때문에 연속적인 행동 공간을 가질 경우 policy-based method인 Policy Gradient 알고리즘을 많이 사용한다.  
PG는 패러미터에 의해 결정되는 확률 분포를 policy $$ \pi_{\theta}(a|s) $$로 정의해서 특정 상태 s에서의 action a를 확률적(stochastic)으로 구하자는 것이다. 

DPG는 SPG 방법만 있는 것이 아니라 deterministic policy로도 policy gradient가 가능하고, model-free 형태여서 이전 policy gradient에서 사용했던 q function의 gradient를 그대로 사용할 수 있다는 점을 보여주었다.
또한, DPG가 SPG의 특수 사례(policy의 분산이 0)임을 밝혔다.





<br>

---

> <subtitle> References </subtitle>

<br>

* [http://proceedings.mlr.press/v32/silver14.pdf](http://proceedings.mlr.press/v32/silver14.pdf){:target="_blank"}
* [https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/){:target="_blank"}
* [https://talkingaboutme.tistory.com/entry/RL-Review-Deterministic-Policy-Gradient-Algorithm](https://talkingaboutme.tistory.com/entry/RL-Review-Deterministic-Policy-Gradient-Algorithm){:target="_blank"}
