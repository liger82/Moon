---
layout: post
title: "[RL] ch06 Temperal Difference Learning [1]"
date: 2021-01-27
excerpt: ""
categories: [RL/RL]
tags : [Reinforcement Learning, RL, Sutton, TD, Q-learning, SNU CML]
comments: true
---

Sutton 교수의 "introduction to reinforcement learning" 교재를 기반으로 공부하다가 참고용으로 듣던 서울대학교 CML 연구실에서 하는 강의가 더 공부하기 좋아서 기준을 바꾸었습니다. 유튜브 주소는 다음과 같습니다.

* [SNU CML 강의 목록 in youtube](https://www.youtube.com/playlist?list=PLKs7xpqpX1beJ5-EOFDXTVckBQFFyTxUH){:target="_blank"}

이 강의가 책을 기반으로 하지만 필요에 따라 책의 챕터를 왔다리 갔다리 해서 TD learning에서 다루는 챕터는 책 기준으로 사실상 6장, 7장, 12장에 걸쳐있습니다. 

<br>

> <subtitle> Intro </subtitle>

* TD는 RL에서 중요하고 비교적 새로운 아이디어
* Monte Carlo와 Dynamic Programming의 장점들을 모은 조합
    - MC의 장단점
        - 장점 : 경험으로부터 학습 가능(model-free method)
        - 단점 : 에피소드가 끝나야 학습 가능. 실시간 학습 불가
    - DP의 장단점
        - 장점 : 최종 결과를 기다리지 않고 추정 가능
        - 단점 : 환경의 동역학이 필요
    - TD : 환경의 동역학 없이 경험으로부터 학습이 가능하면서, 최종 결과를 기다리지 않고 추정치를 업데이트할 수 있다.

<br>

> <subtitle> TD Prediction </subtitle>

# 작성 중 ...

<br>

> <subtitle>  </subtitle>

<br>

> <subtitle>  </subtitle>

<br>

> <subtitle>  </subtitle>

<br>

> <subtitle>  </subtitle>

<br>

> <subtitle>  </subtitle>




<br>

---

> <subtitle> References </subtitle>
* Deep Reinforcement Learning Hands On 2/E Chapter 06 : Deep Q-Networks
* [](){:target="_blank"}

<br>
